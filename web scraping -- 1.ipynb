{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c4220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\navin\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\navin\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\navin\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\navin\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\navin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\navin\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\navin\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\navin\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74cd1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc380d",
   "metadata": {},
   "source": [
    "# Ques No.---1 display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11d14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "\n",
    "# page content\n",
    "soup=BeautifulSoup(page.content)\n",
    "\n",
    "header_tags = []\n",
    "for header in soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"]):\n",
    "    header_tags.append(header.name+ \" \" +header.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa244433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h1 Main Page',\n",
       " 'h1 Welcome to Wikipedia',\n",
       " \"h2 From today's featured article\",\n",
       " 'h2 Did you know\\xa0...',\n",
       " 'h2 In the news',\n",
       " 'h2 On this day',\n",
       " \"h2 Today's featured picture\",\n",
       " 'h2 Other areas of Wikipedia',\n",
       " \"h2 Wikipedia's sister projects\",\n",
       " 'h2 Wikipedia languages',\n",
       " 'h2 Navigation menu',\n",
       " 'h3 Personal tools',\n",
       " 'h3 Namespaces',\n",
       " 'h3 Views',\n",
       " 'h3 Search',\n",
       " 'h3 Navigation',\n",
       " 'h3 Contribute',\n",
       " 'h3 Tools',\n",
       " 'h3 Print/export',\n",
       " 'h3 In other projects',\n",
       " 'h3 Languages']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print all header tags\n",
    "header_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43023837",
   "metadata": {},
   "source": [
    "# Ques no.--2  IMDB's top rated 100 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dbf487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to the web page server to get source code of the page\n",
    "url = \"https://www.imdb.com/list/ls091520106/\"\n",
    "page2 = requests.get(url)\n",
    "\n",
    "# page content\n",
    "soup2 = BeautifulSoup(page2.content)\n",
    "\n",
    "# top Movies name\n",
    "name = soup2.find_all(\"h3\", class_=\"lister-item-header\")\n",
    "# get text from movie name \n",
    "movies_name = [] \n",
    "for i in name:\n",
    "    for j in i.find_all(\"a\"):\n",
    "        movies_name.append(j.text)\n",
    "        \n",
    "        \n",
    "                       \n",
    "# Year of release\n",
    "year = soup2.find_all(\"span\",class_=\"lister-item-year text-muted unbold\")\n",
    "year_of_release = [] \n",
    "for k in year:\n",
    "    a=k.text.replace('(','')\n",
    "    year_of_release.append(a.replace(')','')) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# IMDB Rating\n",
    "rating = soup2.find_all(\"div\",class_=\"ipl-rating-star small\")\n",
    "\n",
    "# rating text\n",
    "IMDB_rating = [] \n",
    "for i in rating:\n",
    "      IMDB_rating.append(float(i.text))    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acdd8313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movies_name</th>\n",
       "      <th>year_of_release</th>\n",
       "      <th>IMDB_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>1959</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>1971</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Snatch</td>\n",
       "      <td>2000</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Le fabuleux destin d'Amélie Poulain</td>\n",
       "      <td>2001</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Kid</td>\n",
       "      <td>1921</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            movies_name year_of_release  IMDB_rating\n",
       "0              The Shawshank Redemption            1994          9.3\n",
       "1                         The Godfather            1972          9.2\n",
       "2                 The Godfather Part II            1974          9.0\n",
       "3                       The Dark Knight            2008          9.0\n",
       "4                          12 Angry Men            1957          9.0\n",
       "..                                  ...             ...          ...\n",
       "95                   North by Northwest            1959          8.3\n",
       "96                   A Clockwork Orange            1971          8.3\n",
       "97                               Snatch            2000          8.2\n",
       "98  Le fabuleux destin d'Amélie Poulain            2001          8.3\n",
       "99                              The Kid            1921          8.3\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make deta frame of 100 movies\n",
    "IMDB_top_100=pd.DataFrame({})\n",
    "IMDB_top_100['movies_name']=movies_name\n",
    "IMDB_top_100['year_of_release']=year_of_release\n",
    "IMDB_top_100['IMDB_rating']=IMDB_rating  \n",
    "IMDB_top_100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8d2e7",
   "metadata": {},
   "source": [
    "# ques no.--3 IMDB's top 100 Indian Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb9de7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.imdb.com/list/ls009997493/\"\n",
    "page3 = requests.get(url)\n",
    "\n",
    "#  page content\n",
    "soup3 = BeautifulSoup(page3.content)\n",
    "\n",
    "# top Movies name\n",
    "name = soup3.find_all(\"h3\", class_=\"lister-item-header\")\n",
    "movies_name = [] \n",
    "for i in name:\n",
    "    for j in i.find_all(\"a\"):\n",
    "        movies_name.append(j.text)\n",
    "\n",
    "\n",
    "# Year of release\n",
    "year = soup3.find_all(\"span\",class_=\"lister-item-year text-muted unbold\")\n",
    "year_of_release = [] \n",
    "\n",
    "for k in year:\n",
    "    a=k.text.replace('(','')\n",
    "    year_of_release.append(a.replace(')','')) \n",
    "    \n",
    "\n",
    "# IMDB Rating\n",
    "rating = soup3.find_all(\"div\",class_=\"ipl-rating-star small\")\n",
    "\n",
    "# rating \n",
    "IMDB_rating = [] #empty list\n",
    "for i in rating:\n",
    "      IMDB_rating.append(float(i.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4656851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movies_name</th>\n",
       "      <th>year_of_release</th>\n",
       "      <th>IMDB_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>2006</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>2009</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>2007</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dil Chahta Hai</td>\n",
       "      <td>2001</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>2004</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Wake Up Sid</td>\n",
       "      <td>2009</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Rangeela</td>\n",
       "      <td>1995</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Shatranj Ke Khilari</td>\n",
       "      <td>1977</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Pyaar Ka Punchnama</td>\n",
       "      <td>2011</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ek Hasina Thi</td>\n",
       "      <td>2004</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               movies_name year_of_release  IMDB_rating\n",
       "0          Rang De Basanti            2006          8.1\n",
       "1                 3 Idiots            2009          8.4\n",
       "2         Taare Zameen Par            2007          8.3\n",
       "3           Dil Chahta Hai            2001          8.1\n",
       "4   Swades: We, the People            2004          8.2\n",
       "..                     ...             ...          ...\n",
       "95             Wake Up Sid            2009          7.6\n",
       "96                Rangeela            1995          7.4\n",
       "97     Shatranj Ke Khilari            1977          7.5\n",
       "98      Pyaar Ka Punchnama            2011          7.6\n",
       "99           Ek Hasina Thi            2004          7.5\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make data frame of top 100 India's IMDB movies\n",
    "indian_top_100=pd.DataFrame({})\n",
    "indian_top_100['movies_name']=movies_name\n",
    "indian_top_100['year_of_release']=year_of_release\n",
    "indian_top_100['IMDB_rating']=IMDB_rating\n",
    "indian_top_100   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c284a8b",
   "metadata": {},
   "source": [
    "# Ques. no.--4 -Write a python program to scrape product name, price and discounts from https://meesho.com/bags\u0002ladies/pl/p7vbp ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef45dbb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_488/2355945100.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Creating data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Product_Name'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mproduct_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Price'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mprice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Discount'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All arrays must be of the same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "url=\"https://www.cnbc.com/world/?region=world\"\n",
    "page4 = requests.get(url)\n",
    "\n",
    "\n",
    "\n",
    "# page content\n",
    "soup4=BeautifulSoup(page4.content)\n",
    "\n",
    "# headline name\n",
    "headline_name = [] # creating empty list\n",
    "for i in soup4.find_all(class_=\"LatestNews-headline\"): \n",
    "    headline_name.append(i.text)\n",
    "\n",
    "# time   \n",
    "time = []\n",
    "for i in soup4.find_all('a',class_=\"LatestNews-wrapper\"): \n",
    "    time.append(i.text)\n",
    "    \n",
    "# scraping news link    \n",
    "news_link = []\n",
    "for i in soup4.find_all('span',class_=\"\"): \n",
    "    news_link.append(i.text) \n",
    "\n",
    "    # Creating data frame \n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Product_Name':product_name,'Price':price,'Discount':discount})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b541b9d",
   "metadata": {},
   "source": [
    "# Ques No.--5  python program to scrape cricket rankings from icc-cricket.com. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1104e",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece57358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "page5 = requests.get(url)\n",
    "\n",
    "#  page5\n",
    "soup5 = BeautifulSoup(page5.content)\n",
    "\n",
    "# team names\n",
    "team = soup5.find_all(\"span\",class_='u-hide-phablet')\n",
    "team_name = []\n",
    "for i in team:\n",
    "    team_name.append(i.text)\n",
    "matches = []\n",
    "points = []\n",
    "ratings = [] \n",
    "new_list = []\n",
    "# Matches\n",
    "for i in soup5.find_all(\"td\",class_='rankings-block__banner--matches'):\n",
    "    matches.append(i.text)\n",
    "    \n",
    "#Team points    \n",
    "for i in soup5.find_all(\"td\",class_='rankings-block__banner--points'):\n",
    "    points.append(i.text)\n",
    "    \n",
    "#Team Ratings    \n",
    "for i in soup5.find_all(\"td\",class_='rankings-block__banner--rating u-text-right'):\n",
    "    ratings.append(i.text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "#  ther teams number of matches and points  \n",
    "    \n",
    "for i in soup5.find_all(\"td\",class_='table-body__cell u-center-text'):\n",
    "    new_list.append(i.text)\n",
    "    \n",
    "for i in range(0,len(new_list)-1,2):\n",
    "    matches.append(new_list[i])\n",
    "    points.append(new_list[i+1])\n",
    "for i in soup5.find_all(\"td\",class_='table-body__cell u-text-right rating'):\n",
    "    ratings.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da574d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame of top 10 ICC teams\n",
    "icc=pd.DataFrame({})\n",
    "icc['Team_name']=team_name[:10]\n",
    "icc['Matches']=matches[:10]\n",
    "icc['Points']=points[:10]\n",
    "icc['Ratings']=ratings[:10]\n",
    "icc  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac92d63",
   "metadata": {},
   "source": [
    "# Ques No.ii)ii) Top 10 ODI Batsmen in men along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "page6 = requests.get(url)\n",
    "#page6\n",
    "soup6 = BeautifulSoup(page6.content)\n",
    "\n",
    "players = [] \n",
    "team_name = [] \n",
    "rating = [] \n",
    "\n",
    "#Player name\n",
    "for i in soup6.find_all(\"div\",class_='rankings-block__banner--name-large'):\n",
    "    players.append(i.text)\n",
    "    \n",
    "#Team name    \n",
    "for i in soup6.find_all(\"div\",class_='rankings-block__banner--nationality'):\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "#Rating    \n",
    "for i in soup6.find_all(\"div\",class_='rankings-block__banner--rating'): # first place player rating\n",
    "    rating.append(i.text)\n",
    "    \n",
    "for i in soup6.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "        \n",
    "for i in soup6.find_all(\"span\",class_='table-body__logo-text'):\n",
    "    team_name.append(i.text)\n",
    "    \n",
    "for i in soup6.find_all(\"td\",class_='table-body__cell rating'): \n",
    "    rating.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data frame of top 10 ICC Batsmen\n",
    "Batsmen=pd.DataFrame({})\n",
    "Batsmen['Player']=players[:10]\n",
    "Batsmen['Team']=team_name[:10]\n",
    "Batsmen['Rating']=rating[:10]\n",
    "Batsmen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f170469",
   "metadata": {},
   "source": [
    "# Ques no.--iii)Top 10 ODI bowlers along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "page7 = requests.get(url)\n",
    "\n",
    "# page7\n",
    "soup7 = BeautifulSoup(page7.content)\n",
    "\n",
    "players = []\n",
    "team_name = []   \n",
    "rating = [] \n",
    "\n",
    "for i in soup7.find_all(\"div\",class_='rankings-block__banner--name-large'):\n",
    "    players.append(i.text)\n",
    "for i in soup7.find_all(\"div\",class_='rankings-block__banner--nationality'): \n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "for i in soup7.find_all(\"div\",class_='rankings-block__banner--rating'):\n",
    "    rating.append(i.text)\n",
    "for i in soup7.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "for i in soup7.find_all(\"span\",class_='table-body__logo-text'):\n",
    "    team_name.append(i.text)\n",
    "for i in soup7.find_all(\"td\",class_='table-body__cell rating'):\n",
    "    rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame of top 10 ICC bowlers\n",
    "bowlers=pd.DataFrame({})\n",
    "bowlers['Player']=players[:10]\n",
    "bowlers['Team']=team_name[:10]\n",
    "bowlers['Rating']=rating[:10]\n",
    "bowlers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1efcd",
   "metadata": {},
   "source": [
    "# Ques No.--6 python program to scrape cricket rankings from ‘www.icc-cricket.com’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced0d0a",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661034f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "page8 = requests.get(url)\n",
    "#page8\n",
    "soup8 = BeautifulSoup(page8.content)\n",
    "\n",
    "# Team names\n",
    "womens_team = soup8.find_all(\"span\",class_='u-hide-phablet')\n",
    "womens_team_name = []\n",
    "for i in womens_team:\n",
    "    womens_team_name.append(i.text)\n",
    "womens_matches = [] \n",
    "womens_points = []\n",
    "womens_ratings = []\n",
    "womens_new_list = []\n",
    "\n",
    "#Matches\n",
    "for i in soup8.find_all(\"td\",class_='rankings-block__banner--matches'):\n",
    "    womens_matches.append(i.text)\n",
    "    \n",
    "#Points    \n",
    "for i in soup8.find_all(\"td\",class_='rankings-block__banner--points'):\n",
    "    womens_points.append(i.text)\n",
    "    \n",
    "#Ratings    \n",
    "for i in soup8.find_all(\"td\",class_='rankings-block__banner--rating u-text-right'):\n",
    "    womens_ratings.append(i.text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "#Women's New List    \n",
    "for i in soup8.find_all(\"td\",class_='table-body__cell u-center-text'):\n",
    "    womens_new_list.append(i.text)\n",
    "    \n",
    "for i in range(0,len(womens_new_list)-1,2):\n",
    "    womens_matches.append(womens_new_list[i])\n",
    "    womens_points.append(womens_new_list[i+1])\n",
    "    \n",
    "for i in soup8.find_all(\"td\",class_='table-body__cell u-text-right rating'):\n",
    "    womens_ratings.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dae73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame of top 10 ICC teams\n",
    "womens_icc=pd.DataFrame({})\n",
    "womens_icc['Team_name']=womens_team_name[:10]\n",
    "womens_icc['Matches']=womens_matches[:10]\n",
    "womens_icc['Points']=womens_points[:10]\n",
    "womens_icc['Ratings']=womens_ratings[:10]\n",
    "womens_icc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b8692",
   "metadata": {},
   "source": [
    "ii)Top 10 women’s ODI players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "page9 = requests.get(url)\n",
    "#Page9\n",
    "soup9 = BeautifulSoup(page9.content)\n",
    "\n",
    "players = [] \n",
    "team_name = [] \n",
    "rating = [] \n",
    "\n",
    "#Players Name\n",
    "for i in soup9.find_all(\"div\",class_='rankings-block__banner--name-large'): # first place player name\n",
    "    players.append(i.text)\n",
    "    \n",
    "#Team Name    \n",
    "for i in soup9.find_all(\"div\",class_='rankings-block__banner--nationality'): # first place player team name\n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "#\n",
    "for i in soup9.find_all(\"div\",class_='rankings-block__banner--rating'): # first place player rating\n",
    "    rating.append(i.text)\n",
    "    \n",
    "#    \n",
    "for i in soup9.find_all(\"td\",class_='table-body__cell rankings-table__name name'):# players name\n",
    "    for j in i.find_all('a'):\n",
    "        \n",
    "#        \n",
    "        players.append(j.text)\n",
    "for i in soup9.find_all(\"span\",class_='table-body__logo-text'): # players team name\n",
    "    team_name.append(i.text)\n",
    "    \n",
    "#    \n",
    "for i in soup9.find_all(\"td\",class_='table-body__cell rating'): # players rating\n",
    "    rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame of top 10 Women's ODI Batting Rankings\n",
    "top_players=pd.DataFrame({})\n",
    "top_players['Player']=players[:10]\n",
    "top_players['Team']=team_name[:10]\n",
    "top_players['Rating']=rating[:10]\n",
    "top_players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecba759",
   "metadata": {},
   "source": [
    "iii)Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "page10 = requests.get(url)\n",
    "\n",
    "#Page10\n",
    "soup10 = BeautifulSoup(page10.content)\n",
    "\n",
    "\n",
    "players = [] \n",
    "team_name = []   \n",
    "rating = []\n",
    "\n",
    "#Players Name\n",
    "for i in soup10.find_all(\"div\",class_='rankings-block__banner--name-large'): \n",
    "    players.append(i.text)\n",
    "    \n",
    "#Team Name    \n",
    "for i in soup10.find_all(\"div\",class_='rankings-block__banner--nationality'): \n",
    "    team_name.append(i.text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "#Rating    \n",
    "for i in soup10.find_all(\"div\",class_='rankings-block__banner--rating'): \n",
    "    rating.append(i.text)\n",
    "    \n",
    "#    \n",
    "for i in soup10.find_all(\"td\",class_='table-body__cell rankings-table__name name'):\n",
    "    for j in i.find_all('a'):\n",
    "        players.append(j.text)\n",
    "        \n",
    "#        \n",
    "for i in soup10.find_all(\"span\",class_='table-body__logo-text'): \n",
    "    team_name.append(i.text)\n",
    "    \n",
    "#    \n",
    "for i in soup10.find_all(\"td\",class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame of top 10 ICC Women's ODI All-Rounder Rankings\n",
    "all_rounder=pd.DataFrame({})\n",
    "all_rounder['Player']=players[:10]\n",
    "all_rounder['Team']=team_name[:10]\n",
    "all_rounder['Rating']=rating[:10]\n",
    "all_rounder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba441fc1",
   "metadata": {},
   "source": [
    "# Ques No.--9  Python program to scrape mentioned details from ‘https://www.dineout.co.in/delhi-restaurants/buffetspecial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b879b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request to the webpage server\n",
    "page = requests.get(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")\n",
    "# page content\n",
    "soup=BeautifulSoup(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17666f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[]\n",
    "for i in soup.find_all(\"div\", class_=\"restnt-info cursor\"):\n",
    "    name.append(i.text)\n",
    "location=[]\n",
    "for i in soup.find_all(\"div\", class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "\n",
    "price = []\n",
    "cuisine = []\n",
    "for i in soup.find_all(\"span\", class_=\"double-line-ellipsis\"):\n",
    "    price.append(i.text.split('|')[0])\n",
    "    cuisine.append(i.text.split('|')[1])\n",
    "\n",
    "rating=[]\n",
    "for i in soup.find_all(\"div\", class_=\"restnt-rating rating-3\"):\n",
    "    rating.append(i.text)\n",
    "for i in soup.find_all(\"div\", class_=\"restnt-rating rating-4\"):\n",
    "    rating.append(i.text)\n",
    "\n",
    "images = []\n",
    "for i in soup.find_all(\"img\", class_=\"no-img\"):\n",
    "    images.append(i['data-src'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c028b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(name), len(location), len(price), len(cuisine), len(rating), len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ce441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data frame\n",
    "DineOut=pd.DataFrame({})\n",
    "DineOut['Restaurant Name']=name\n",
    "DineOut['Location']=location\n",
    "DineOut['Price']=price \n",
    "DineOut['Cuisine']=cuisine  \n",
    "DineOut['Rating']=rating  \n",
    "DineOut['IMAGES']=images\n",
    "DineOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79ab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
